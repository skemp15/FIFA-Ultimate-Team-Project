{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df_train = pd.read_csv(r'datasets\\train_dataset.csv')\n",
    "df_test = pd.read_csv(r'datasets\\test_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a training a test datset ready to to build a model. However, we have a lot of features so we will likely have an overfit model as it stands. Therefore we will need to reduce our number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first build and evaluate a basic model to act as our baeline: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split our training dataset further into a training a validation set to avoid data leakage: \n",
    "df_train_val, df_test_val = train_test_split(df_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define X and y\n",
    "X_train_val = df_train_val.drop(['Price'], axis=1)\n",
    "X_test_val = df_test_val.drop(['Price'], axis=1)\n",
    "\n",
    "y_train_val = df_train_val['Price']\n",
    "y_test_val = df_test_val['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.5182466122875551\n"
     ]
    }
   ],
   "source": [
    "# Create and fit model\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test_val)\n",
    "\n",
    "# Evaluate\n",
    "r2 = r2_score(y_test_val, y_pred)\n",
    "print('R2 Score:', r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty bad performance metric but we should be able to increase it through reducing our number of features. We will intially do this by reducing any feature that doesn't correlate sufficiently with our target variable Price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price                 1.000000\n",
      "Rating                0.481858\n",
      "Dribbling             0.334290\n",
      "Base Stats            0.312157\n",
      "In Game Stats         0.309657\n",
      "Passing               0.288767\n",
      "Team_paris sg         0.285507\n",
      "Pace                  0.262575\n",
      "Skills                0.258870\n",
      "Team_Other            0.254120\n",
      "Shooting              0.246417\n",
      "Weak Foot             0.230601\n",
      "Team_fut icons        0.218905\n",
      "League_icons          0.218905\n",
      "Card Type_if          0.207927\n",
      "Card Type_icon        0.160509\n",
      "Popularity            0.148952\n",
      "League_rem_eur_div    0.101327\n",
      "WR_Att_OE             0.099537\n",
      "LW                    0.093860\n",
      "Name: Price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# View correlation values\n",
    "top_corr_vals = abs(df_train_val.corr()['Price']).sort_values(ascending=False)\n",
    "print(top_corr_vals.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all of our features have fairly low correlations values with our target variable, we will set our limit fairly low:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous number of features: 81\n",
      "New number of features: 14\n"
     ]
    }
   ],
   "source": [
    "# Set a correlation limit\n",
    "corr_limit = 0.2\n",
    "\n",
    "# Get a list of features with a correlation over corr_limit\n",
    "corr_over_lim = top_corr_vals[top_corr_vals > corr_limit].index.to_list()\n",
    "corr_over_lim.remove('Price')\n",
    "\n",
    "# Reduce features of df_train and df_test\n",
    "X_train_val2 = X_train_val[corr_over_lim]\n",
    "X_test_val2 = X_test_val[corr_over_lim]\n",
    "\n",
    "# Show previous and new number of features\n",
    "print('Previous number of features:', X_train_val.shape[1])\n",
    "print('New number of features:', X_train_val2.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also reduce one of any two features that have a correlation with one another of over 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous number of features: 14\n",
      "New number of features: 13\n",
      "Dropped features: ['League_icons']\n"
     ]
    }
   ],
   "source": [
    "# Define colinearity limit \n",
    "colin_limit = 0.8\n",
    "\n",
    "# Create a correlation matrix\n",
    "corr_matrix = X_train_val2.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than 0.8\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > colin_limit)]\n",
    "\n",
    "# Drop the highly correlated features\n",
    "X_train_val3 = X_train_val2.drop(to_drop, axis=1)\n",
    "X_test_val3 = X_test_val2.drop(to_drop, axis=1)\n",
    "\n",
    "# Show previous and new number of features\n",
    "print('Previous number of features:', X_train_val2.shape[1])\n",
    "print('New number of features:', X_train_val3.shape[1])\n",
    "\n",
    "# Show dropped columns\n",
    "print('Dropped features:', to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rerun our baseline model to see if we get better results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.5714809669861545\n"
     ]
    }
   ],
   "source": [
    "# Create and fit model\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train_val3, y_train_val)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test_val3)\n",
    "\n",
    "# Evaluate\n",
    "r2 = r2_score(y_test_val, y_pred)\n",
    "print('R2 Score:', r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this has marginally improved the score, but we can build a function that will allow us to compare performance scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_colin_limits(df_train_val, df_test_val, corr_limit, colin_limit):\n",
    "\n",
    "    # Find top correlation values   \n",
    "    top_corr_vals = abs(df_train_val.corr()['Price']).sort_values(ascending=False)\n",
    "\n",
    "    # Get a list of features with a correlation over corr_limit\n",
    "    corr_over_lim = top_corr_vals[top_corr_vals > corr_limit].index.to_list()\n",
    "\n",
    "    # Reduce features of df_train and df_test\n",
    "    df_train_val2 = df_train_val[corr_over_lim]\n",
    "    df_test_val2 = df_test_val[corr_over_lim]\n",
    "\n",
    "    # Create a correlation matrix\n",
    "    corr_matrix = df_train_val2.corr().abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Find features with correlation greater than 0.8\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > colin_limit)]\n",
    "\n",
    "    # Drop the highly correlated features\n",
    "    df_train_val3 = df_train_val2.drop(to_drop, axis=1)\n",
    "    df_test_val3 = df_test_val2.drop(to_drop, axis=1)\n",
    "\n",
    "    # Define X and y\n",
    "    X_train = df_train_val3.drop('Price', axis=1)\n",
    "    X_test = df_test_val3.drop('Price', axis=1)\n",
    "\n",
    "    y_train = df_train_val3['Price']\n",
    "    y_test = df_test_val3['Price']\n",
    "\n",
    "    # Create and fit model\n",
    "    rf = RandomForestRegressor()\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then run this on a range of correlation and colinearity thresholds and find the best performing combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a range of correlation and colinearity limits:\n",
    "corr_lims = np.linspace(0.1, 0.3, 21)\n",
    "colin_lims = np.linspace(0.5,1,11)\n",
    "\n",
    "# Create am empty dictionary to store values\n",
    "r2_dict = {}\n",
    "\n",
    "# Loop through limits and add values to the dictionary\n",
    "for corr in corr_lims:\n",
    "    r2_scores = []\n",
    "    num_feat_removed = [] \n",
    "    for colin in colin_lims:\n",
    "        r2 = test_colin_limits(df_train, df_test, corr_limit, colin)\n",
    "        r2_scores.append(r2)\n",
    "    r2_dict[corr] = r2_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest R2 value is 0.538 when the correlation limit is 0.13 and colinearity limit is 0.65\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe of r2 scores\n",
    "r2_df = pd.DataFrame(r2_dict, index=colin_lims)\n",
    "\n",
    "# Find the highest r2 score and its location in the dataframe\n",
    "highest_r2 = r2_df.values.max()\n",
    "highest_r2_loc = np.unravel_index(r2_df.values.argmax(), r2_df.shape)\n",
    "\n",
    "# Get the values of the optimal correlation and colinearity limits\n",
    "best_corr_lim = corr_lims[highest_r2_loc[1]]\n",
    "best_colin_lim = colin_lims[highest_r2_loc[0]]\n",
    "\n",
    "print(f'The highest R2 value is {highest_r2:.3f} when the correlation limit is {best_corr_lim:.2f} and colinearity limit is {best_colin_lim:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good increase in performance so we will apply these limits before looking into building and tuning some additional models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets again:\n",
    "df_train = pd.read_csv(r'datasets\\train_dataset.csv')\n",
    "df_test = pd.read_csv(r'datasets\\test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous number of features: 81\n",
      "New number of features: 11\n",
      "\n",
      "\n",
      "Remaining features: ['Rating', 'Dribbling', 'Base Stats', 'Team_paris sg', 'Pace', 'Skills', 'Team_Other', 'Weak Foot', 'Team_fut icons', 'Card Type_if', 'Popularity']\n"
     ]
    }
   ],
   "source": [
    "# Set correlation and colin limits:\n",
    "corr_limit = best_corr_lim\n",
    "colin_limit = best_colin_lim\n",
    "\n",
    "# Get a list of features with a correlation over corr_limit\n",
    "corr_over_lim = top_corr_vals[top_corr_vals > corr_limit].index.to_list()\n",
    "\n",
    "# Reduce features of df_train and df_test\n",
    "df_train2 = df_train[corr_over_lim]\n",
    "df_test2 = df_test[corr_over_lim]\n",
    "\n",
    "# Create a correlation matrix\n",
    "corr_matrix = df_train2.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than 0.8\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > colin_limit)]\n",
    "\n",
    "# Drop the highly correlated features\n",
    "df_train3 = df_train2.drop(to_drop, axis=1)\n",
    "df_test3 = df_test2.drop(to_drop, axis=1)\n",
    "\n",
    "# Show previous and new number of features\n",
    "print('Previous number of features:', df_train.shape[1]-1)\n",
    "print('New number of features:', df_train3.shape[1]-1)\n",
    "\n",
    "# Show remaining features\n",
    "rem_features = df_train3.columns.to_list()\n",
    "rem_features.remove('Price')\n",
    "print('\\n')\n",
    "print('Remaining features:', rem_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train3.to_csv(r'datasets\\train_dataset_reduced_features.csv', index=False)\n",
    "df_test3.to_csv(r'datasets\\test_dataset_reduced_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have removed quite a lot of features so we can always go back and reduce out correlation and colinearity limits, but for now we can try building different models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
